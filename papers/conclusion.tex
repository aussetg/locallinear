\section{CONCLUSION}\label{sec:conclusion}
In this paper, we have studied the estimator of the (supposedly sparse) gradient of the regression function obtained by solving a regularized local linear version of the $k$-NN problem with a $\ell_1$ penalty. Nonasymptotic bounds for the local estimation error have been established, improving upon those obtained for alternative methods in sparse situations.
We derived non-asymptotic error bounds on a local linear estimator of the gradient based on $k$-nearest neighbours averaging and with a sparsity inducing $L^1$ penalty. Compared to previous similar estimators we show that exploiting the sparsity of the gradient improves convergence rates.
Beyond its theoretical properties and its computational simplicity, the local estimation method promoted here is shown to be the key ingredient for designing efficient algorithms for variable selection and $M$-estimation, as supported by various numerical experiments. Hopefully, this work shall pave the way to the elaboration of novel statistical learning procedures that exploits the local structure of the gradient, and for which the theory will be extended to take into account the underlying geometry of the space in order to obtain convergence rates depending only on the true intrinsic dimension of the data such as done in~\cite{mukherjeeLearningGradientsManifolds2010} for the kernel smoothing setting.
