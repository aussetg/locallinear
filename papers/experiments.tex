\section{NUMERICAL EXPERIMENTS}\label{sec:exp}

In order to motivate the need for a robust estimator of the gradient, we introduce three different examples of use of our estimator compared to existing approaches. All the code to reproduce the experiments and figures can be found at \url{https://git.sr.ht/~aussetg/locallinear}.

As our estimator is sensitive to the choice of hyperparameters $k$ and $\lambda$ we use a local leave-one-out procedure described in Algorithm~\ref{alg:localcv} for hyperparameter selection. As only the regression variable $Y$ is observed, the regression error is used as a proxy loss in the cross-validation. 
The high cost of $k$-NN is amortized by using $k$-d trees, bringing the total average complexity of the nearest neighbour search down to $O(n \log n)$. In cases where the aforementioned cost is too high ($n$ in the order of millions) it is possible to instead make use of approximate nearest neighbour schemes such as HNSW~\citep{malkovEfficientRobustApproximate2020}. Approximate Nearest Neighbours algorithms have recently enjoyed a regain of interest and provide high accuracy at a very low computational cost \citep{aumullerANNBenchmarksBenchmarkingTool2018}.
\begin{algorithm}
    \caption{Local Leave-One-Out}\label{alg:localcv}
    \begin{algorithmic}[1] %
        \Require $x$: sample point, $(X, Y)$: training set, $(K, \Lambda)$: grid
        \State{$X_{\text{LoO}} \gets \texttt{Neighbourhood of } x \texttt{ in } X \texttt{ of size } N$}
        \For{$k \in K, \lambda \in \Lambda$}
            \For{$X_i \in X_{\text{LoO}}$}
                \State{$m_i,  \beta_i \gets $ \texttt{estimated gradient at} $X_i$ \texttt{w.r.t} $X, Y$ \texttt{using}~\eqref{lll}}
            \EndFor
            \State{$\texttt{error}_{k, \lambda} \gets \frac{1}{N} \sum_{i=1}^N (m_i - Y_i)^2$}
        \EndFor
        \State{$k^\star, \lambda^\star \gets \argmin_{k, \lambda} \texttt{error}_{k, \lambda}$}
        \State{\textbf{return} $k^\star, \lambda^\star$}
    \end{algorithmic}
\end{algorithm}





\subsection{Variable Selection}

While a large number of observations is desirable the same is not necessarily the case for the individual features; a large number of features can be detrimental to the computational performance of most learning methods but also harmful to the predictive performance. In order to mitigate the detrimental impact of the high dimensionality, or \emph{curse of dimensionality}, one can try to reduce the effective dimension of the problem. A large body of work exists on dimensionality reduction as a preprocessing step that considers the intrinsic dimensionality of $X$ by considering for example that $X$ lies on a lower-dimensional manifold. Those approaches only consider $X$ in isolation and do not take into account $Y$ which is the variable of interest. It is possible to use the information in $Y$ to direct the dimension reduction of $X$, either by treating $Y$ as side information, as is done in~\cite{bachPredictiveLowrankDecomposition2005}, or by considering the existence of an explicit \emph{index space} such that $Y_i = g(v_1^\intercal X_i, \cdots, v_m^\intercal X_i) + \varepsilon_i$ as is done in~\cite{dalalyanNewAlgorithmEstimating2008}. In the latter case, it is possible to observe that the $\emph{index space}$ lies on the subspace spanned by the gradient.

In contrast with the work of~\cite{dalalyanNewAlgorithmEstimating2008} our approach is local and it is therefore possible to retrieve a different subspace in different regions of $\mathbb{R}^D$. As localizing the estimator increases its variance, we choose to only identify the dimensions of interest instead of estimating the full projection matrix.
We introduce \emph{Gradient Guided Trees} in Algorithm~\ref{alg:LocalLinearTree} to exploit the local aspect of our estimator in order to direct the cuts in a random tree: at each step, cuts are drawn randomly with probability proportional to estimated mean absolute gradient in the cell.
\begin{algorithm}
    \caption{Node Splitting for Gradient Guided Trees}\label{alg:LocalLinearTree}
    \begin{algorithmic}[1] %
        \Require $(X, Y)$: training set, $\texttt{Node}$: indexes of points in the node
        \State{$\nabla m (X_i) \gets \texttt{estimated gradient at } X_i, \, \forall i \in \texttt{Node}$ \texttt{ using }~\eqref{lll}}
        \State{$\omega \gets \sum_{i \in \texttt{Node}} \lvert \nabla m(X_i) \rvert$}
        \State{$K \gets$ \texttt{sample} $\sqrt{D}$ \texttt{dimensions in} $\{1, \ldots, d\} \texttt{ with probability weights} \propto \omega$}
        \State{$k, c \gets \texttt{best threshold } c \texttt{ and dimension } k$}
        \State{$\textbf{return } k, c$}
    \end{algorithmic}
\end{algorithm}
We demonstrate the improvements brought by guiding the cuts by the local information provided by the gradient by comparing the performance of a vanilla regression random forest with the same procedure but with local gradient information. 
We consider five datasets: the Breast Cancer Wisconsin (Diagnostic) Data Set introduced in~\cite{streetNuclearFeatureExtraction1993}; the Heart Disease dataset introduced by~\cite{detranoInternationalApplicationNew1989}; the classic Diamonds Price dataset; the Gasoline NIR dataset introduced by~\cite{kalivasTwoDataSets1997} and the Sloan Digital Sky Survey DR14 dataset of~\cite{abolfathiFourteenthDataRelease2018}. We measure the $L^2$ loss by cross validation across 50 folds using the same hyperparameters for the growing of the forest in both the standard and gradient guided variants.

We denote by RF, Random Forests grown from standard CART trees while GGF denote \emph{Gradient Guided Forests} grown from the \emph{Gradient Guided Trees} previously introduced. As seen in Table~\ref{table:results}, gradient guided split sampling consistently outperform the vanilla variant. When all variables are relevant, as is the case when the variables were carefully selected by the practitioner with prior knowledge, our variant performs similarly to the original algorithm while performance is greatly improved when only a few variables are relevant, such as in the NIR dataset~\citep{portierBootstrapTestingRank2014}.
\begin{table}
    \centering
    \begin{tabular}{@{}lrrrr@{}}
        & \multicolumn{2}{c}{Description} & \multicolumn{2}{c}{Loss} \\
        \cmidrule(l){2-3} \cmidrule(l){4-5}
        Dataset & $n$ & $D$ & RF & GGF \\
        \midrule
        Wisconsin & $569$ & $30$ & \thead{$0.0352$ \\ $\pm 3.29\cdot 10^{-4}$} & \thead{$\mathbf{0.0345}$ \\ $\pm 3.35 \cdot 10^{-4}$} \\
        Heart & $303$ & $13$ & \thead{$0.128$ \\ $\pm 6.6\cdot 10^{-4}$} & \thead{$\mathbf{0.124}$ \\ $\pm 8.6\cdot 10^{-4}$} \\
        Diamonds & $53940$ & $23$ & \thead{$680033$ \\ $\pm 3.45\cdot 10^{9}$} & \thead{$\mathbf{664265}$ \\ $\pm 2.81\cdot 10^{9}$} \\
        Gasoline & $60$ & $401$ & \thead{$0.678$ \\ $\pm 0.451$} & \thead{$\mathbf{0.512}$ \\ $\pm 0.347$} \\
        SDSS & $10000$ & $8$ & \thead{$0.872\cdot 10^{-3}$ \\ $\pm 4.50\cdot 10^{-6}$} & \thead{$\mathbf{0.776}\cdot 10^{-3}$ \\ $\pm 6.00\cdot 10^{-6}$} \\
        \bottomrule
    \end{tabular}
    \caption{Performance of the two random forest algorithms on a $50$-folds cross validation.}\label{table:results}
\end{table}


\subsection{Gradient Free Optimization}

Many of the recent advances in the field of machine learning have been made possible in one way or another by advances in optimization; both in how well we are able to optimize complex function and what type of functions we are able to optimize if only locally. Recent advances in automatic differentiation as well as advances that push the notion of \emph{what} can be differentiated have given rise to the notion of \emph{differentiable programming}~\citep{innesDifferentiableProgrammingSystem2019} in which a significant body of work can be expressed as the solution to a minimization problem usually then solved by gradient descent.

We study here the use of the local linear estimator of the gradient in Algorithm~\ref{alg:lolamin} in cases where analytic or automatic differentiation is impossible, and compare it to a standard gradient free optimization technique as well as the oracle where the true gradient is known.
\begin{algorithm}
    \caption{Estimated Gradient Descent}\label{alg:lolamin}
    \begin{algorithmic}[1] %
        \Require $x_0$: initial guess, $f$: function $\mathbb{R}^D \to \mathbb{R}$, $M$: budget
        \State{$X \gets X_1, \ldots, X_M \texttt{ with } X_i \sim \mathcal{N}(x_0, \varepsilon \times I_D)$}\label{alg:lolamin:line1}
        \State{$Y \gets f(X) := f(X_1), \ldots, f(X_M)$}
        \While{\texttt{not StoppingCondition}}
            \State{$m, \Delta \gets$ \texttt{estimated gradient at} $x$ \texttt{w.r.t} $X, Y$ \texttt{using}~\eqref{lll}}\label{alg:lolamin:line5}
            \State{$X \gets X, X_1, \ldots, X_M \texttt{ with } X_i \sim \mathcal{N}(\texttt{GradientStep}(x, \Delta), \varepsilon \times I_D)$}
            \State{$Y \gets f(X)$}
            \State{$x \gets \argmin_{X_i} \{ f(X_i) \}$}
        \EndWhile
        \State{$\textbf{return } x$}
    \end{algorithmic}
\end{algorithm}
While line~\texttt{1} bears resemblance with Gaussian smoothing and could therefore be seen as analogous to gradient estimation via Gaussian smoothing (see~\cite{berahasTheoreticalEmpiricalComparison2020}), two key differences here are the subsequent local linear step as well as the fact that the samples from line~\ref{alg:lolamin:line1} are not necessarily the samples used in the local linear estimator of line~\ref{alg:lolamin:line5}. 

We first minimize the standard but challenging Rosenbrock function for different values of $d$. which is defined as
\begin{align}
    f(x) = 100 \sum_{i=1}^{d-1} (x_{i+1} - x_i)^2 + (x_i - 1)^2.
\end{align}
We compare for reference our approach to the Nelder-Mead (simplex search) algorithm; a standard gradient free optimization technique.
It is apparent in Figure~\ref{fig:rosenbrock_grad} that estimating the gradient yields a significant advantage compared to traditional gradient-free techniques that usually have to rely on bounding arguments and feasible regions and therefore scale unfavourably with the dimension. As our approach uses a nearest neighbours formulation for the gradient estimate, we are able to efficiently reuse past samples in the current estimate of the gradient; this makes it possible to achieve a sufficiently accurate estimate of the gradient even in high dimensions.
\begin{figure}
    \centering
    \begin{subfigure}[t]{1.\linewidth}
        \centering
        \input{figs/rosenbrock_50.tikz} %
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{1.\linewidth}
        \centering
        \input{figs/rosenbrock_100.tikz} %
    \end{subfigure}
    \caption{Nesterov Gradient Descent on the rosenbrock function for $d=50$ (top) and $d=100$ (bottom).}\label{fig:rosenbrock_grad}
\end{figure}
\footnotetext{The number of function evaluations does not have any meaning for the true gradient. We use here that $1$ estimated gradient step $\approx$ $50$ function evaluations. $5000$ function evaluations therefore equate to $100$ gradient steps.}
We compare in Figure~\ref{fig:rosenbrock_vs} the approach developed previously to the estimators proposed by~\cite{wangStochasticZerothorderOptimization2018} and~\cite{fanDesignadaptiveNonparametricRegression1992}. As the approach proposed by~\cite{wangStochasticZerothorderOptimization2018} includes the use of \emph{mirror descent}, for fairness, we have implemented our proposed gradient descent algorithm of Algorithm~\ref{alg:lolamin} using our estimator as well as those of~\cite{wangStochasticZerothorderOptimization2018} and~\cite{fanLocalLinearRegression1993} (with reuse of previous samples where appropriate) for the gradient. We then reimplemented the mirror descent algorithm of~\cite{wangStochasticZerothorderOptimization2018} with the previous estimators of the gradient. We observe in Figure~\ref{fig:rosenbrock_vs} that our method compares favourably: our estimator is able to reuse past samples in its gradient estimation and has therefore access to a better gradient estimate for a fixed, given number of function evaluations.
\begin{figure}
    \centering
    \begin{subfigure}[t]{1.\linewidth}
        \centering
        \input{figs/rosenbrock_100+wang_nesterov_fixed.tikz}
    \end{subfigure}%
    \hfill
    \begin{subfigure}[t]{1.\linewidth}
        \centering
        \input{figs/rosenbrock_100+wang_md_fixed.tikz}
    \end{subfigure}%
    \caption{Nesterov Gradient Descent (top) and Mirror Gradient Descent (bottom) on the Rosenbrock function for $d=100$.}\label{fig:rosenbrock_vs}
\end{figure}
We apply the previous method to the minimization of the log-likelihood of a logistic model on the UCI's Adult data set, consisting of $48842$ observations and $14$ attributes amounting to $101$ dimensions once one-hot encoded and an intercept added.
\begin{multline}
    \mathcal{L}_\theta (X) =  -\sum_i Y_i \log (1 + \exp (-\theta X_i)) \\
    -(1 - Y_i) \log (1 + \exp (\theta X_i)), \; \theta \in \mathbb{R}^{101}.
\end{multline}
We also compare the effective CPU wall time needed to reach a given log-likelihood in order to give a more comprehensive view of the relative performance of the multiple algorithms. Given that the time per iteration can vary greatly depending on the cost of evaluations and the cost of the gradient procedures, it is important to use both the number of evaluations and the time metric jointly with the former being more relevant as the cost of individual function evaluations increases.
\begin{figure}
    \centering
    \begin{subfigure}[b]{1.\linewidth}
        \centering
        \input{figs/logisticreg.tikz} %
    \end{subfigure}%
    \vfill
    \begin{subfigure}[b]{1.\linewidth}
        \centering
        \input{figs/logisticreg_time.tikz} %
    \end{subfigure}%
    \caption{Log-likelihood of the logistic regression on a test set, trained by Nesterov Gradient Descent with respect to the number of evaluations (top) and time (bottom).}
\end{figure}

\subsection{Disentanglement}

\emph{Disentangled representation learning} aims to learn a representation of the input space such that the independent dimensions of the representation each encode separate but meaningful attributes of the original feature space.
If the space of interest is the space of \emph{faces}, a disentangled representation would then for example be a lower-dimensional space where one dimension encodes the sex of the subject, another its age, and so forth. 
We show here how our estimator can be useful for retrieving the dimensions associated with a concept in a supervised manner.

A $\beta$-VAE~\citep{higginsBetaVAELearningBasic2017} model is trained on the \texttt{CACD2000} dataset of celebrity faces with age labels to first build low-dimensional a representation of the images and then extract the direction relating to age. We learn $\mathcal{E}_{\phi}$ and $\mathcal{D}_{\theta}$ parameterizing $q_\phi$ and $p_\theta$, to minimize the loss
\begin{multline}
    \mathcal{L} (\theta, \phi; x, z, \beta) = \mathbb{E}_{q_\phi (z \mid x)} \left[ \log p_\theta (x \mid z) \right] \\
    - \beta \infdiv{q_\phi x}{x},
\end{multline}
where $\beta$ acts as a constraint on the representational power of the latent distribution; $\beta = 1$ leads to the standard VAE formulation of~\cite{kingmaAutoEncodingVariationalBayes2014} while $\beta > 1$ increases the level of disentanglement. We use a standard symmetrical encoder-decoder architecture for the variational autoencoder, schematically presented in Figure~\ref{fig:vae}. All the relevant implementation details can be found in the \texttt{Julia} code in the supplementary materials.
\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figs/vae.pdf}
    \caption{Encoder-Decoder Architecture used for this work}\label{fig:vae}
\end{figure*}
We learn a $512$-dimensional representation of the $128\times 128$ images and encode all the \texttt{CACD2000} images.
Once all the images have been encoded in $\mathbb{R}^{512}$ it is possible to use the local linear estimator of the gradient studied in this work to derive the gradient of the age with respect to the latent variable, making it possible to produce a new version of the input image that appears either older or younger as done in Figure~\ref{fig:age}. By computing a local estimate of the gradient, we are able to derive a more meaningful change when the age is not perfectly disentangled.
\begin{figure}[H]
    \centering
    \begin{tikzpicture}
        \node[inner sep=0pt] (encoded) at (0,0)
            {\includegraphics[width=.1\textwidth]{figs/encoded.jpg}};
        \node[inner sep=0pt] (aged) at (6,0)
            {\includegraphics[width=.1\textwidth]{figs/aged.jpg}};
        \draw[->,thick] (encoded.east) -- (aged.west)
            node[midway,fill=white] {$z + 0.1 \times \nabla m(z)$};
    \end{tikzpicture}
    \caption{Extracting the direction of interest for aging.}\label{fig:age}
\end{figure}
Note that the quality of the image reconstruction and generation is here solely limited by the choice of the encoding and decoding model and is not related to the methods introduced in this paper, significant advances in the quality of the decoding have been made in the recent years and if a better quality and less blurry decoded output is desired we encourage the reader to replace the decoder with a \texttt{PixelCNN} architecture such as presented in~\cite{salimansPixelCNNImprovingPixelCNN2017}. The quality of the gradient is also significantly impacted by the quality of the annotations as \texttt{CACD200} is an automatically annotated and noisy dataset. 

Using our estimator it is possible to estimate the gradient $\nabla m$ of $\mathbb{E} [ Y \mid Z = z ]$ with respect to the latent variable $Z$ (illustrated in the Appendix). It is then possible to analyse the sparsity of $\nabla m$ to quantify the quality of the disentanglement for varying level of $\beta$ by quantifying how far from a single dimension the gradient for the age is concentrated. As the true dimension is unknown, we instead measure the angular distance to all dimensions reweighted by the magnitudes of the partial derivatives:
\begin{equation}
    \begin{split}
        &\sum_i \frac{\lvert\hat \nabla_i m(x) \rvert}{\lvert \hat \nabla m(x) \rvert} \cos (e_i, \frac{1}{n} \sum_k \lvert \hat \nabla m(x) \rvert), \\
        \text{where} \quad &\cos (a, b) = \frac{a \cdot b}{\lVert a \rVert \lVert b \rVert}.
    \end{split}
\end{equation}
We observe in Figure~\ref{fig:disentangle} that as $\beta$ increases the age slowly become disentangled, as expected if one considers the age to be an important and independent characteristic of human faces.
\begin{figure}[H]
    \centering
    \input{figs/disentangle.tikz}
    \caption{Quality of disentanglement with respect to the age} \label{fig:disentangle}
\end{figure}

While not an entirely adequate metric for disentanglement, not only because disentanglement does not necessarily require the dimensions to be the one an observer expected but more importantly because this metric requires an annotated dataset; we believe this metric can be useful for practitioners. By measuring how close the estimated gradients are to the axis, with respect to an annotated dataset of characteristics of interest, a practitioner can ensure his model is sufficiently disentangled for downstream tasks such as face manipulation by a user. We also believe it is possible to design an end-to-end differentiable framework in order to force disentanglement to consider the characteristics of interest: our estimator is the solution to a convex optimization program and as such admits an adjoint; it is therefore possible to fit a local linear estimator inside an automatic differentiation framework such as done in~\cite{agrawalDifferentiableConvexOptimization2019}.
