\section{INTRODUCTION}

In this paper, we place ourselves in the usual regression setup, one of the flagship predictive problems in statistical learning. Here and throughout, $(X,Y)$ is a pair of random variables defined on the same probability space $(\Omega,\; \mathcal{F},\; \mathbb{P})$ with unknown probability distribution $P$: the r.v. $Y$ is real valued and square integrable, whereas the (supposedly continuous) random vector $X$ takes its values in $\mathbb{R}^D$, with $D\geq 1$, and models some information \textit{a priori} useful to predict $Y$. Based on a sample $\mathcal{D}_n=\{ (X_1,Y_1),\; \ldots,\; (X_n,Y_n) \}$ of $n\geq 1$ independent copies of the generic pair $(X,Y)$, the goal pursued is to build a Borelian mapping $f:\mathbb{R}^d\rightarrow \mathbb{R}$ that produces, on average, a good prediction $f(X)$ of $Y$. Measuring classically its accuracy by the squared error, the learning task then boils down to finding a predictive function $f$ that is solution of the risk minimization problem $\min_{f}\mathcal{R}_P(f)$, where 
\begin{equation}\label{eq:true_risk}
\mathcal{R}_P(f)=\mathbb{E}\left[\left(Y-f(X)\right)^2\right].
\end{equation}
Of course, the minimum is attained by the regression function $m(X)=\mathbb{E}[Y\mid X]$, which is unknown, just like $Y$'s conditional distribution given $X$ and the risk \eqref{eq:true_risk}. The empirical risk minimization (ERM) strategy consists in solving the optimization problem above, except that the unknown distribution $P$ is replaced by an empirical estimate based on the training data $\mathcal{}D_n$, such as the raw empirical distribution $\hat{P}_n=(1/n)\sum_{i\leq n}\delta_{X_i}$ typically, denoting by $\delta_x$ the Dirac mass at any point $x$, and minimization is restricted to a class $\mathcal{F}$ supposed to be rich enough to include a reasonable approximant of $m$ but not too complex (\textit{e.g.} of finite {\sc VC} dimension) in order to control the fluctuations of the deviations between the empirical and true distributions uniformly over it. Under the assumption that the random variables $Y$ and $f(X)$, $f\in \mathcal{F}$, have sub-Gaussian tails, the analysis of the performance of empirical risk minimizers (\textit{i.e.} predictive functions obtained by least-squares regression) has been the subject of much interest in the literature, see \textit{e.g.} \cite{gyorfiDistributionFreeTheoryNonparametric2002}, \cite{massartConcentrationInequalitiesModel2007}, \cite{boucheronConcentrationInequalitiesNonasymptotic2013} or \cite{lecueLearningSubgaussianClasses2016} (and refer to \textit{e.g.} \cite{lugosiRiskMinimizationMedianofmeans2016} for alternatives to the ERM approach in non sub-Gaussian situations).

In this paper, we are interested in estimating accurately the (supposedly well-defined) gradient $\nabla m(x)$ by means of the popular $k$ nearest neighbour ($k$-NN) approach, see \textit{e.g.} Chapter in \cite{devroyeProbabilisticTheoryPattern1996a} or \cite{biauLecturesNearestNeighbor2015}. The \textit{gradient learning} issue has received increasing attention in the context of local learning problems such as classification or regression these last few years, see \textit{e.g.} \cite{mukherjeeEstimationGradientsCoordinate2006,mukherjeeLearningCoordinateCovariances2006}. Because it provides a valuable information about the local structure of a dataset in a high-dimensional space, an accurate estimator of the gradient of a predictive function can be used for various purposes such as dimensionality reduction or variable selection (see \textit{e.g.}~\cite{hristacheStructureAdaptiveApproach2001, hristacheDirectEstimationIndex1998,xiaAdaptiveEstimationDimension2002,xiaConstructiveApproachEstimation2007,dalalyanNewAlgorithmEstimating2008,yeLearningSparseGradients2012, trivediConsistentEstimatorExpected2014}), the partial derivative w.r.t. a given variable being a natural indicator of its importance regarding prediction. The previous references are all concerned with outer products of gradients so as to recover some dimension-reduction subspace.
Estimators of the gradients have also been proposed for zeroth-order optimization (see \textit{e.g.}~\cite{nesterovRandomGradientFreeMinimization2017,wangStochasticZerothorderOptimization2018,berahasTheoreticalEmpiricalComparison2020}) and can benefit from good convergence properties.

Whereas the use of standard nonparametric methods for gradient estimation is documented in the literature (see~\cite{fanLocalPolynomialModelling1996,delecroixNonparametricEstimationRegression1996,debrabanterDerivativeEstimationLocal2013} for the use of local polynomial with kernel smoothing techniques, \cite{gasserEstimatingRegressionFunctions1984} for the so-called Gasser-Muller alternative, \cite{zhouDerivativeEstimationSpline2000} for the use of regression spline and \cite{mukherjeeLearningCoordinateCovariances2006} for the estimation on a reproducing kernel Hilbert space with kernel smoothing), it is the purpose of the present article to investigate the performance of an alternative local averaging method, the popular $k$-NN method. As it provides piecewise constant estimates, it is easier to conceptualize for the practitioner and, more importantly; the neighbourhoods determined by the parameter $k$ are data-driven and often more consistent than those defined by the bandwidth in the kernel setting, especially in high dimensions.

Here we investigate the behaviour of the estimator of the (supposedly sparse) gradient of the regression function at a given point $x\in \mathbb{R}^D$, obtained by solving a regularized local linear version of the $k$-NN problem with a Lasso penalty. Precisely, nonasymptotic bounds for the related estimation error are established. Whereas $k$-NN estimators of the regression function have been extensively analysed (see \textit{e.g.}~\cite{biauRatesConvergenceFunctional2010,kpotufeKNNRegressionAdapts2011,jiangNonAsymptoticUniformRates2019} and the references therein), the result stated in this paper is the first of this type to the best of our knowledge. 

The relevance of the approach promoted is then illustrated by several applications. A variable selection algorithm that exploits the local nature of the gradient estimator proposed is first exploited to refine the popular random forest algorithm (see \cite{breimanRandomForests2001a}): by exploiting the node estimate of the gradient we are able to direct better the choice of cuts. Very simple to implement and accurate, as supported by the various numerical experiments carried out, it offers an attractive and flexible alternative to existing traditional methods such as PCA or the more closely related method of \cite{dalalyanNewAlgorithmEstimating2008}, allowing for a local reduction of the dimension rather than implementing a global preprocessing of the data.
We next show how a rough statistical estimate of the gradient of any smooth objective function based on the estimation principle previously analysed in the context of regression can be exploited in a basic gradient descent algorithm. We exploit the local structure of the algorithm to be able to reuse past computations in order to calculate our estimator and jump to a better local minimum at each gradient step as well.
Finally, we give an example of the usefulness of a sparse gradient estimate when the gradient is believed to be truly sparse: we use our estimator to retrieve the direction of interest for a specific attribute inside a disentangled representation and show how this can be used as an \textit{ad hoc} measure of disentanglement.

The article is organized as follows. In section~\ref{sec:background}, the estimation method and the assumptions involved in the subsequent analysis are listed. The main theoretical results of the paper are stated in section~\ref{sec:main}, while several applications of the estimation method promoted are described at length and illustrated by numerical experiments in section~\ref{sec:exp}. Some concluding remarks are collected in section~\ref{sec:conclusion} and technical proofs are postponed to the Supplementary Material.
